# Deep Neural Networks with PyTorch
## IBM

Week 1 - Tensor and Datasets

1.1 - Tensors 1D

1.2 - Two-Dimensional Tensors

1.3 - Derivatives in PyTorch

1.4 - Simple Dataset

1.5 - Dataset

Week 2 - Linear Regression, Linear Regression PyTorch Way

2.1 - Linear Regression in 1D - Prediction

2.2 - Linear Regression Training

2.3 - Gradient Descent and Cost

2.4 - PyTorch Slope

2.5 - Linear Regression Training

***** Linear Regression PyTorch Way *****

3.1 - Stochastic Gradient Descent and the Data Loader

3.2 - Mini-Batch Gradient Descent

3.3 - Optimization in Pytorch

3.4 - Training, Validation and Test Split

Week 3 - Multiple Input Output Linear Regression, Logistic Regression for Classification

4.1 - Multiple Linear Regression Prediction

4.2 - Multiple Output Linear Regression

***** Logistic Regression for Classification *****

5.0 - Linear Classifier and Logistic Regression

5.1 - Logistic Regression Prediction

5.2 - Bernoulli Distribution Maximum Likelihood Estimation

5.3 - logistic Regression Cross Entropy Loss

Week 4 - Softmax Regression, Shallow Neural Networks

6.1 - Softmax Prediction

6.2 - Softmax Function

6.3 - Softmax PyTorch

***** Shallow Neural Networks

7.1 - Neural Networks in One Dimension

7.2 - Neural Networks More Hidden Neurons

7.3 - Neural Networks with Multiple Dimensional Input

7.4 - Multi-Class Neural Networks

7.5 - Backpropagation

7.6 - Avtivation Functions

Week 5 - Deep Networks

8.1 - Deep Neural Networks

8.2 - Dropout

8.3 - Neural Network initialization weights

8.4 - Gradient Descent with Momentum

8.5* - Batch Normalization

Week 6 - Convolutional Neural Network

9.1 - Convolution

9.2 - Activation Functions and Max Polling

9.3 - Multiple Input and Output Channels

9.4 - Convolutional Neural Network

9.5 - Torch-Vision Models


Week 7 - Peer Review 
- Fashion MNIST Classification Assignment
